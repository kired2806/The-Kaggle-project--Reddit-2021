{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline \n",
    "import os\n",
    "import matplotlib as mpl\n",
    "from pandas import Series\n",
    "import re\n",
    "#Pas de message d'alertes\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Any results you write to the current directory are saved as output.\n",
    "df = pd.read_csv(\"../input/train.csv\")  \n",
    "xtest=pd.read_csv('../input/xtest.csv')\n",
    "\n",
    "\n",
    "#Lire 5 ligne de manière aléa \n",
    "df.sample(5)\n",
    "\n",
    "\n",
    "\n",
    "#Regardons la taille  de notre dataset\n",
    "df.dtypes\n",
    "\n",
    "\n",
    "\n",
    "#convertir le type de author en object \n",
    "df['author']=df['author'].astype('object')\n",
    "\n",
    "df.columns\n",
    "\n",
    "\n",
    "#Nous allons supprimer la variables entities \n",
    "df.drop(columns=['entities', 'Unnamed: 0'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#Regardons les vleurs manquantes \n",
    "df.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "#Statistiques descriptives des variables quantitatives \n",
    "df_qual=df.select_dtypes(exclude=['object'])\n",
    "df_qual.describe().plot(kind = \"area\",fontsize=22, figsize = (18,8), table = True,colormap=\"rainbow\")\n",
    "plt.xlabel('',)\n",
    "plt.ylabel('Value')\n",
    "plt.title(\"Statistiques générales des variables \")\n",
    "\n",
    "df1=df\n",
    "df1['author']=df1['author'].astype('object')\n",
    "df1['author'][df1['author'] == '0' ] = 'Trump'\n",
    "df1['author'][df1['author'] == '1' ] = 'Clinton'\n",
    "f,ax=plt.subplots(1,2,figsize=(16,7))\n",
    "df['author'].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\n",
    "ax[0].set_title('df1 author Count')\n",
    "ax[0].set_ylabel('Count')\n",
    "sns.countplot('author',data=df,ax=ax[1])\n",
    "ax[1].set_title('df1 author Count')\n",
    "plt.show()\n",
    "#Description des variables quantitatives \n",
    "df.select_dtypes(exclude='object').describe()\n",
    "\n",
    "\n",
    "#description des variables quantitatives \n",
    "df.select_dtypes(include='object').describe()\n",
    "\n",
    "\n",
    "\n",
    "df2=df[['retweet_count', 'favorite_count', 'author']]\n",
    "df2['author'] = df2['author'].astype('object')\n",
    "df2['author'][df2['author'] == 1 ] = 'Trump'\n",
    "df2['author'][df2['author'] == 0 ] = 'Clinton'\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,7)\n",
    "fig=sns.stripplot(x='author',\n",
    "                  y='favorite_count',data=df2,jitter=True,\n",
    "                  edgecolor='gray',size=8,palette='winter',orient='v')\n",
    "\n",
    "df2=df[['retweet_count', 'favorite_count', 'author']]\n",
    "df2['author'] = df2['author'].astype('object')\n",
    "df2['author'][df2['author'] == 1 ] = 'Trump'\n",
    "df2['author'][df2['author'] == 0 ] = 'Clinton'\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,7)\n",
    "fig=sns.stripplot(x='author',\n",
    "                  y='retweet_count',data=df2,jitter=True,\n",
    "                  edgecolor='gray',size=8,palette='winter',orient='v')\n",
    "\n",
    "\n",
    "#Allos nous allons définir un new data appelé data qui contient uniquement lentgh, text et author \n",
    "data=df[['author', 'text', 'favorite_count']] \n",
    "data['author'][data['author'] == 1 ] = 'Trump'\n",
    "data['author'][data['author'] == 0 ] = 'Clinton'\n",
    "\n",
    "\n",
    "\n",
    "data.sample(5)\n",
    "\n",
    "\n",
    "\n",
    "#Regardons est ce que la taille des twett peut nous indiquer sa  provenence \n",
    "sns.factorplot('author','favorite_count',data=df)\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Regardone la distribution des length en fonction des candidants \n",
    "mpl.rcParams['patch.force_edgecolor'] = True\n",
    "plt.style.use('seaborn-bright')\n",
    "df.hist(column='favorite_count', by='author', bins=50,figsize=(11,5))\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "def cleanedWords(raw_sentence):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_sentence)\n",
    "    words = letters_only.lower().split()                            \n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    meaningful_words = [w for w in words\n",
    "                       if w not in stops]\n",
    "    return meaningful_words\n",
    "\n",
    "\n",
    "#sms=tweet\n",
    "#création de la base tweet\n",
    "tweet=data[['text','author']]\n",
    "#fonction de code en binaire 0/1\n",
    "def transformSpamColumn(x):\n",
    "    if x=='Clinton':\n",
    "        return 0\n",
    "    return 1\n",
    "#appication de la fonction sur tweet['author']\n",
    "tweet['author']=tweet['author'].apply(transformSpamColumn)\n",
    "\n",
    "def getWordCloud(data, author):\n",
    "    data=data[data['author'] == author]\n",
    "    words = ' '.join(df['text'])\n",
    "    cleaned_word = \" \".join(cleanedWords(words))\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color='black',\n",
    "                      width=3000,\n",
    "                      height=2500\n",
    "                     )\n",
    "    wordcloud.generate(cleaned_word)\n",
    "    plt.figure(1,figsize=(12, 12))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "#les mots les plus fréquent de Trump\n",
    "getWordCloud(tweet,1)\n",
    "\n",
    "#Les mots plus fréquents de Clinton\n",
    "getWordCloud(tweet,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Partie Modélisation¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split \n",
    "#from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "import calendar\n",
    "import datetime\n",
    "import re\n",
    "messages = df[['author','text']]\n",
    "test = xtest[['text']] \n",
    "messages.columns = ['label', 'message']\n",
    "test.columns = ['message']\n",
    "\n",
    "\n",
    "print(messages[:5])\n",
    "\n",
    "\n",
    "\n",
    "def split_into_tokens(message):\n",
    "    message = message  # convert bytes into proper unicode\n",
    "    return TextBlob(message).words\n",
    "\n",
    "\n",
    "\n",
    "messages.message.head()\n",
    "\n",
    "messages.message.apply(split_into_tokens).head()\n",
    "\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "def split_into_lemmas(message):\n",
    "    message = message.lower()\n",
    "    words = TextBlob(message).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in words]\n",
    "\n",
    "messages.message.apply(split_into_lemmas).head()\n",
    "\n",
    "\n",
    "\n",
    "bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(messages['message'])\n",
    "print(len(bow_transformer.vocabulary_))\n",
    "print(bow_transformer.get_feature_names()[:5])\n",
    "\n",
    "messages_bow = bow_transformer.transform(messages['message'])\n",
    "print('sparse matrix shape:', messages_bow.shape)\n",
    "print('number of non-zeros:', messages_bow.nnz)\n",
    "print('sparsity: %.2f%%' % (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1])))\n",
    "\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "\n",
    "\n",
    "\n",
    "print (tfidf_transformer.idf_[bow_transformer.vocabulary_['the']])\n",
    "print (tfidf_transformer.idf_[bow_transformer.vocabulary_['hannity']])\n",
    "\n",
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "print(messages_tfidf.shape)\n",
    "\n",
    "\n",
    " messages['label'] = messages['label']\n",
    " messages['label'] =  messages['label'].astype('int64')\n",
    "\n",
    "\n",
    "\n",
    "spam_detector = SVC(kernel='sigmoid', gamma=1.0).fit(messages_tfidf,  messages['label'])\n",
    "\n",
    "\n",
    "\n",
    "all_predictions = spam_detector.predict(messages_tfidf)\n",
    "\n",
    "\n",
    "\n",
    "tr_acc = accuracy_score(messages['message'], all_predictions)\n",
    "#print(\"Accuracy on training set:  %.2f%%\" % (100 * tr_acc))\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=split_into_lemmas)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', SVC(kernel='sigmoid', gamma=1.0)),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])   \n",
    "scores = cross_val_score(pipeline,  # steps to convert raw messages into models\n",
    "                         messages['message'], # training data\n",
    "                         messages['label'],  # training labels\n",
    "                         cv=10,  # split data randomly into 10 parts: 9 for training, 1 for scoring\n",
    "                         scoring='accuracy',  # which scoring metric?\n",
    "                         n_jobs=-1,  # -1 = use all cores = faster\n",
    "                         )\n",
    "print(scores)\n",
    "\n",
    "print(classification_report(messages['label'], all_predictions))\n",
    "\n",
    "print('Mean score:', scores.mean(), '\\n')\n",
    "print('Stdev:', scores.std())\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "params = {\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'bow__analyzer': (split_into_lemmas, split_into_tokens),\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline,  # pipeline from above\n",
    "    params,  # parameters to tune via cross validation\n",
    "    refit=True,  # fit using all available data at the end, on the best found param combination\n",
    "    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy',  # what score are we optimizing?\n",
    "    cv=StratifiedKFold( n_splits=5),  # what type of cross validation to use\n",
    ")\n",
    "\n",
    "%time nb_detector = grid.fit(messages['message'], messages['label'])\n",
    "print(nb_detector.cv_results_)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "nb_detector.cv_results_\n",
    "\n",
    "\n",
    "predictions = nb_detector.predict(test['message'])\n",
    "\n",
    "print(predictions.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "predictions = pd.Series(predictions)\n",
    "predictions.index = range(predictions.shape[0])\n",
    "predictions.to_csv(\"predictions.csv\", index_label =\"index\", header = [\"prediction\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTRE EXEMPLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 886kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 1.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/5b/bd0f0fb5564183884d8e35b81d06d7ec06a20d1a0c8b4c407f1554691dce/joblib-1.0.0-py3-none-any.whl (302kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 1.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/28/ff0d0936a31f15a0879caf6dac1f1cbaab1fc7b9e8baf8a1d5a70380fb22/regex-2020.11.13-cp37-cp37m-manylinux2010_x86_64.whl (667kB)\n",
      "\u001b[K     |████████████████████████████████| 675kB 936kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/02/8f8880a4fd6625461833abcf679d4c12a44c76f9925f92bf212bb6cefaad/tqdm-4.56.0-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/sid31/snap/jupyter/6/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, joblib, regex, tqdm, nltk\n",
      "Successfully installed click-7.1.2 joblib-1.0.0 nltk-3.5 regex-2020.11.13 tqdm-4.56.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/33/87b15a5baeeb71bd677da3579f907e97476c5247c0e56a37079843af5424/pandas-1.2.2-cp37-cp37m-manylinux1_x86_64.whl (9.9MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9MB 1.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.5 in /home/sid31/snap/jupyter/common/lib/python3.7/site-packages (from pandas) (1.18.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /snap/jupyter/6/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Collecting pytz>=2017.3 (from pandas)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/94/784178ca5dd892a98f113cdd923372024dc04b8d40abe77ca76b5fb90ca6/pytz-2021.1-py2.py3-none-any.whl (510kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 1.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /snap/jupyter/6/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.12.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.2.2 pytz-2021.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import nltk.corpus# sample text for performing tokenization\n",
    "from nltk.tokenize import word_tokenize# Passing the string text into word tokenize for breaking the sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the eastern side of South America\"\n",
    "# importing word_tokenize from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1056)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/sid31/snap/jupyter/6/nltk_data'\n    - '/snap/jupyter/6/nltk_data'\n    - '/snap/jupyter/6/share/nltk_data'\n    - '/snap/jupyter/6/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-fb8de01b049b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/sid31/snap/jupyter/common/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sid31/snap/jupyter/common/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sid31/snap/jupyter/common/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sid31/snap/jupyter/common/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sid31/snap/jupyter/common/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/sid31/snap/jupyter/6/nltk_data'\n    - '/snap/jupyter/6/nltk_data'\n    - '/snap/jupyter/6/share/nltk_data'\n    - '/snap/jupyter/6/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "token = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding frequency distinct in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a6e51fb29c61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Importing FreqDist library from nltk and passing token into FreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'token' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# finding the frequency distinct in the tokens\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(token)\n",
    "fdist\n",
    "\n",
    "# To find the frequency of top 10 words\n",
    "fdist1 = fdist.most_common(10)\n",
    "fdist1\n",
    "\n",
    "\n",
    "# Importing Porterstemmer from nltk library\n",
    "# Checking for the word ‘giving’ \n",
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "pst.stem('waiting')\n",
    "\n",
    "\n",
    "# Checking for the list of words\n",
    "stm = [\"waited\", \"waiting\", \"waits\"]\n",
    "for word in stm :\n",
    "    print(word+ \":\" +pst.stem(word))\n",
    "\n",
    "\n",
    "# Importing LancasterStemmer from nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "stm = ['giving', 'given', 'given', 'gave']\n",
    "for word in stm :\n",
    "    print(word+ \":\" +lst.stem(word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
